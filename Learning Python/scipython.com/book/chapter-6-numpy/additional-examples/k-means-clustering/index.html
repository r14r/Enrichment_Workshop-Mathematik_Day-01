<!doctype html>
<html lang="en">


<head>
<meta http-equiv="Content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="keywords" content="">
<meta name="description" content="The algorithmThe $k$-means clustering method is a popular algorithm for partitioning a data set into &quot;clusters&quot; in which each data point is assigned to the cluster with the nearest mean. It will be illustrated here with a data set of $n$ points, each of $m=2$ dimensions: $X=[(x_1^{(1)}, x_1^{(2)}), (x_2^{(1)}, x_2^{(2)}), \cdots, (x_n^{(1)}, x_n^{(2)})]$, plotted below.">
<title>$k$-means clustering</title>
<link rel="shortcut icon" href="/static/img/favicon.ico">


<link rel="alternate" type="application/rss+xml" title="RSS" href="/blog/feeds/rss/">
<link rel="alternate" type="application/atom+xml" title="Atom" href="/blog/feeds/atom/">



<link rel="stylesheet" href="/static/css/bootstrap.css">
<link rel="stylesheet" href="/static/css/mezzanine.css">
<link rel="stylesheet" href="/static/css/codehilite.css">
<link rel="stylesheet" href="/static/css/custom.css">






<script src="/static/mezzanine/js/jquery-1.8.3.min.js"></script>
<script src="/static/js/bootstrap.js"></script>
<script src="/static/js/bootstrap-extras.js"></script>



<!--[if lt IE 9]>
<script src="/static/js/html5shiv.js"></script>
<script src="/static/js/respond.min.js"></script>
<![endif]-->



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


</head>

<body id="body">

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
<div class="container">
<div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <span class="sr-only">Toggle Navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
    </button>
    
    
</div>
<div class="navbar-collapse collapse">
    
<form action="/search/" class="navbar-form navbar-right" role="search">

<div class="form-group">
    <input class="form-control" placeholder="Search" type="text" name="q" value="">
</div>


    
    <div class="form-group">
    <select class="form-control" name="type">
        <option value="">Everything</option>
        
        <option value="blog.BlogPost"
            >
            Blog posts
        </option>
        
        <option value="pages.Page"
            >
            Pages
        </option>
        
    </select>
    </div>
    


<input type="submit" class="btn btn-default" value="Go">

</form>

    
<ul class="nav navbar-nav"><li id="dropdown-menu-home"><a href="/">Home</a></li><li class="dropdown
               "
        id="about"><a href="/about/"
        
        class="dropdown-toggle disabled" data-toggle="dropdown"
        >
            About
            <b class="caret"></b></a><ul class="dropdown-menu"><li class="
               "
        id="about-the-author"><a href="/about/the-author/">The Author</a></li><li class="
               "
        id="about-the-book"><a href="/about/the-book/">The Book</a></li><li class="
               "
        id="about-this-website"><a href="/about/this-website/">This Website</a></li></ul></li><li class="dropdown
               active"
        id="book"><a href="/book/"
        
        class="dropdown-toggle disabled" data-toggle="dropdown"
        >
            Book
            <b class="caret"></b></a><ul class="dropdown-menu"><li class="
               "
        id="book-chapter-1-introduction"><a href="/book/chapter-1-introduction/">Chapter 1: Introduction</a></li><li class="dropdown-submenu
               "
        id="book-chapter-2-the-core-python-language-i"><a href="/book/chapter-2-the-core-python-language-i/">Chapter 2: The Core Python Language I</a><ul class="dropdown-menu"><li class="
               "
        id="book-chapter-2-the-core-python-language-i-examples"><a href="/book/chapter-2-the-core-python-language-i/examples/">Examples</a></li><li class="
               "
        id="book-chapter-2-the-core-python-language-i-questions"><a href="/book/chapter-2-the-core-python-language-i/questions/">Questions</a></li><li class="
               "
        id="book-chapter-2-the-core-python-language-i-questions-problems"><a href="/book/chapter-2-the-core-python-language-i/questions/problems/">Problems</a></li><li class="
               "
        id="book-chapter-2-the-core-python-language-i-additional-problems"><a href="/book/chapter-2-the-core-python-language-i/additional-problems/">Additional Problems</a></li></ul></li><li class="dropdown-submenu
               "
        id="book-chapter-3-simple-plotting-with-pylab"><a href="/book/chapter-3-simple-plotting-with-pylab/">Chapter 3: Simple Plotting with pylab</a><ul class="dropdown-menu"><li class="
               "
        id="book-chapter-3-simple-plotting-with-pylab-examples"><a href="/book/chapter-3-simple-plotting-with-pylab/examples/">Examples</a></li><li class="
               "
        id="book-chapter-3-simple-plotting-with-pylab-problems"><a href="/book/chapter-3-simple-plotting-with-pylab/problems/">Problems</a></li></ul></li><li class="dropdown-submenu
               "
        id="book-chapter-4-the-core-python-language-ii"><a href="/book/chapter-4-the-core-python-language-ii/">Chapter 4: The core Python language II</a><ul class="dropdown-menu"><li class="
               "
        id="book-chapter-4-the-core-python-language-ii-examples"><a href="/book/chapter-4-the-core-python-language-ii/examples/">Examples</a></li><li class="
               "
        id="book-chapter-4-the-core-python-language-ii-questions"><a href="/book/chapter-4-the-core-python-language-ii/questions/">Questions</a></li><li class="
               "
        id="book-chapter-4-the-core-python-language-ii-problems"><a href="/book/chapter-4-the-core-python-language-ii/problems/">Problems</a></li><li class="
               "
        id="book-chapter-4-the-core-python-language-ii-additional-problems"><a href="/book/chapter-4-the-core-python-language-ii/additional-problems/">Additional Problems</a></li></ul></li><li class="dropdown-submenu
               "
        id="book-chapter-5-ipython-and-ipython-notebook"><a href="/book/chapter-5-ipython-and-ipython-notebook/">Chapter 5: IPython and IPython Notebook</a><ul class="dropdown-menu"><li class="
               "
        id="book-chapter-5-ipython-and-ipython-notebook-examples"><a href="/book/chapter-5-ipython-and-ipython-notebook/examples/">Examples</a></li></ul></li><li class="dropdown-submenu
               active"
        id="book-chapter-6-numpy"><a href="/book/chapter-6-numpy/">Chapter 6: NumPy</a><ul class="dropdown-menu"><li class="
               "
        id="book-chapter-6-numpy-examples"><a href="/book/chapter-6-numpy/examples/">Examples</a></li><li class="
               "
        id="book-chapter-6-numpy-questions"><a href="/book/chapter-6-numpy/questions/">Questions</a></li><li class="
               "
        id="book-chapter-6-numpy-problems"><a href="/book/chapter-6-numpy/problems/">Problems</a></li><li class="
               active"
        id="book-chapter-6-numpy-additional-examples"><a href="/book/chapter-6-numpy/additional-examples/">Additional Examples</a></li><li class="
               "
        id="book-chapter-6-numpy-additional-problems"><a href="/book/chapter-6-numpy/additional-problems/">Additional Problems</a></li></ul></li><li class="dropdown-submenu
               "
        id="book-chapter-7-matplotlib"><a href="/book/chapter-7-matplotlib/">Chapter 7: Matplotlib</a><ul class="dropdown-menu"><li class="
               "
        id="book-chapter-7-matplotlib-examples"><a href="/book/chapter-7-matplotlib/examples/">Examples</a></li><li class="
               "
        id="book-chapter-7-matplotlib-questions"><a href="/book/chapter-7-matplotlib/questions/">Questions</a></li><li class="
               "
        id="book-chapter-7-matplotlib-problems"><a href="/book/chapter-7-matplotlib/problems/">Problems</a></li></ul></li><li class="dropdown-submenu
               "
        id="book-chapter-8-scipy"><a href="/book/chapter-8-scipy/">Chapter 8: SciPy</a><ul class="dropdown-menu"><li class="
               "
        id="book-chapter-8-scipy-examples"><a href="/book/chapter-8-scipy/examples/">Examples</a></li><li class="
               "
        id="book-chapter-8-scipy-questions"><a href="/book/chapter-8-scipy/questions/">Questions</a></li><li class="
               "
        id="book-chapter-8-scipy-problems"><a href="/book/chapter-8-scipy/problems/">Problems</a></li><li class="
               "
        id="book-chapter-8-scipy-additional-examples"><a href="/book/chapter-8-scipy/additional-examples/">Additional Examples</a></li><li class="
               "
        id="book-chapter-8-scipy-additional-problems"><a href="/book/chapter-8-scipy/additional-problems/">Additional Problems</a></li></ul></li><li class="dropdown-submenu
               "
        id="book-chapter-9-general-scientific-programming"><a href="/book/chapter-9-general-scientific-programming/">Chapter 9: General Scientific Programming</a><ul class="dropdown-menu"><li class="
               "
        id="book-chapter-9-general-scientific-programming-examples"><a href="/book/chapter-9-general-scientific-programming/examples/">Examples</a></li><li class="
               "
        id="book-chapter-9-general-scientific-programming-questions"><a href="/book/chapter-9-general-scientific-programming/questions/">Questions</a></li><li class="
               "
        id="book-chapter-9-general-scientific-programming-problems"><a href="/book/chapter-9-general-scientific-programming/problems/">Problems</a></li></ul></li><li class="
               "
        id="book-errata"><a href="/book/errata/">Errata</a></li></ul></li><li class="
               "
        id="blog"><a href="/blog/"
        >
            Blog
            
        </a></li><li class="
               "
        id="apps"><a href="/apps/"
        >
            Apps
            
        </a></li><li class="
               "
        id="contact"><a href="/contact/"
        >
            Contact
            
        </a></li></ul>

</div>
</div>
</div>

<div class="container">





<ul class="breadcrumb">
<li id="breadcrumb-menu-home"><a href="/">Home</a></li><li id="breadcrumb-menu-book"><a href="/book/">Book</a></li><li id="breadcrumb-menu-book-chapter-6-numpy"><a href="/book/chapter-6-numpy/">Chapter 6: NumPy</a></li><li id="breadcrumb-menu-book-chapter-6-numpy-additional-examples"><a href="/book/chapter-6-numpy/additional-examples/">Additional Examples</a></li><li id="breadcrumb-menu-book-chapter-6-numpy-additional-examples-k-means-clustering"
        class="active">$k$-means clustering</li>
</ul>

<h1>
$k$-means clustering</h1>

</div>

<div class="container">
<div class="row">

<div class="col-md-12 middle" style="max-width: 760px;">
    

<h3 id="the-algorithm">The algorithm</h3>
<p>The $k$-means clustering method is a popular algorithm for partitioning a data set into "clusters" in which each data point is assigned to the cluster with the nearest mean. It will be illustrated here with a data set of $n$ points, each of $m=2$ dimensions: $X=[(x_1^{(1)}, x_1^{(2)}), (x_2^{(1)}, x_2^{(2)}), \cdots, (x_n^{(1)}, x_n^{(2)})]$, plotted below.</p>
<p><img alt="enter image description here" src="/static/media/uploads/blog/kmeans/unclustered-data.png"></p>
<p>To the human eye, it would seem reasonable to assign these points to $K=3$ clusters. The algorithm proceeds by first picking three random points as the centroids of the clusters:</p>
<p><img alt="enter image description here" src="/static/media/uploads/blog/kmeans/initial-centroids.png"></p>
<p>We then iterate over two steps. First assign each data point to the cluster with the nearest centroid:</p>
<p><img alt="enter image description here" src="/static/media/uploads/blog/kmeans/kmeans-00.png"></p>
<p>Next, move each centroid to the mean value of the points assigned to it:</p>
<p><img alt="enter image description here" src="/static/media/uploads/blog/kmeans/kmeans-01.png"></p>
<p>On the next iteration, the each data point is again assigned to the cluster with the nearest centroid (since they have moved, this generally means reassigning some of the data points):</p>
<p><img alt="enter image description here" src="/static/media/uploads/blog/kmeans/kmeans-02.png"></p>
<p>and again, the centroids are moved to the new mean value of each cluster:</p>
<p><img alt="enter image description here" src="/static/media/uploads/blog/kmeans/kmeans-03.png"></p>
<p>This procedure is replaced until the centroids no longer move because no data points are reassigned. The final cluster assignments for the initial conditions are shown below.</p>
<p><img alt="enter image description here" src="/static/media/uploads/blog/kmeans/kmeans-04.png"></p>
<h3 id="mathematical-details">Mathematical details</h3>
<p>Mathematically, the $k$-means algorithm partitions the data set into $K$ sets, $\boldsymbol{C} = {C_1, C_2, \cdots, C_K}$, such that the sum of the within-cluster squared distances between each data point and the cluster mean is minimized:</p>
<p>$$
\underset{\boldsymbol{C}}{\arg \min} \sum_{k=1}^K \sum_{\boldsymbol{x} \in C_k} ||\boldsymbol{x} - \boldsymbol{\mu_k}||^2
$$ </p>
<p>The $k$-means method is not guaranteed to find the global minimum in this equation and so it is frequently implemented by re-running it several times for different random initial choices for $\boldsymbol{\mu}$ and selecting the clustering with the smallest value of the above summation.</p>
<h3 id="python-implementation">Python implementation</h3>
<p><strong>Notes</strong></p>
<p>The cluster assignment step is carried out with this line of code:</p>
<div class="codehilite"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">([(</span><span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">av</span><span class="p">)</span> <span class="err">@</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">av</span><span class="p">))</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">av</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mu</span><span class="p">)],</span>
                    <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">e</span><span class="p">:</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<p>The squared distance between data point $\boldsymbol{x}$  (<code>x</code>) and the mean of cluster $i$, $\boldsymbol{\mu}_i$ (<code>av = mu[idx]</code>) is calculated as the vector dot product, <code>(x-av) @ (x-av)</code> implemented with Python's new <a href="https://www.python.org/dev/peps/pep-0465/"><code>@</code> infix operator</a>. This expression can be replaced with <code>np.dot((x-av), (x-av))</code> in versions of Python earlier than 3.5. The list comprehension then generates the index into the cluster average array and value of the squared distance between each data point and that cluster average. From this list, the index with the minimum distance is returned and set to <code>k</code>: that is, the index of the cluster with the smallest distance between its mean and the data point.</p>
<p>The cluster means (centroids) are moved in another single line of code:</p>
<div class="codehilite"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">clusters</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mu</span><span class="p">))]</span>
</pre></div>


<p>This simply resets the list of vectors <code>mu</code> to the average value of each cluster's data points. The argument <code>axis=0</code> ensures we average over the observations but not over each dimension of the data vectors.</p>
<p><strong>Warning</strong></p>
<p>This code is an illustration of a vectorized implementation of the $k$-means algorithm only and for study by people learning Python and NumPy. It is not optimized implementation of the algorithm for "production" use: for that, consider using the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans"><code>KMeans</code></a> class from the <a href="http://scikit-learn.org/stable/"><code>scikit-learn</code></a> Python machine learning package.</p>
<p><strong>The code</strong></p>
<p>The first function defined, <code>kmeans</code>, performs a single run of the $k$-means algorithm and partitions the provided data, <code>X</code>, into <code>nclusters</code> ($K$) clusters. The second function, <code>kmeans_pp</code> runs <code>kmeans</code> a given number of times, <code>nrepeat</code>, and returns the optimum clustering found over these runs. </p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">kmeans</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">nclusters</span><span class="p">):</span>
    <span class="sd">"""Perform k-means clustering with nclusters clusters on data set X.</span>

<span class="sd">    Returns mu, an ordered list of the cluster centroids and clusters, a</span>
<span class="sd">    list of nclusters lists containing the clustered points from X.</span>

<span class="sd">    X is an array of of shape (n,m) containing n data points (observations)</span>
<span class="sd">    each of dimension m.</span>
<span class="sd">    nclusters (also commonly referred to as K) is the number of clusters to</span>
<span class="sd">    partition the data into.</span>

<span class="sd">    """</span>

    <span class="k">def</span> <span class="nf">assign_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
        <span class="sd">"""Assign points from X to clusters centered on points mu."""</span>

        <span class="c1"># The clustered points will be put in m independent lists in clusters:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
        <span class="n">clusters</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)]</span>
        <span class="c1"># Assign each data point (observation) to a cluster indexed by k such</span>
        <span class="c1"># that the squared distance between the observation and that cluster's</span>
        <span class="c1"># mean is a minimum.</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">([(</span><span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">av</span><span class="p">)</span> <span class="err">@</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">av</span><span class="p">))</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">av</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mu</span><span class="p">)],</span>
                    <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">e</span><span class="p">:</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">clusters</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">clusters</span>

    <span class="k">def</span> <span class="nf">move_centroids</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">clusters</span><span class="p">):</span>
        <span class="sd">"""Move the centroids of each cluster to that cluster's average."""</span>

        <span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">clusters</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mu</span><span class="p">))]</span>
        <span class="k">return</span> <span class="n">mu</span>

    <span class="k">def</span> <span class="nf">is_converged</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">oldmu</span><span class="p">):</span>
        <span class="sd">"""Return True if the centroids mu and oldmu are the same.</span>

<span class="sd">        If the centroid positions are the same the clustering has converged.</span>

<span class="sd">        """</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">oldmu</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Pick some random data points as the initial cluster centroids.</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nclusters</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">),:]</span>
    <span class="c1"># Iterate the k-means algorithm to convergence.</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">oldmu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="n">clusters</span> <span class="o">=</span> <span class="n">assign_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">move_centroids</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">clusters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_converged</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">oldmu</span><span class="p">):</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">clusters</span>

<span class="k">def</span> <span class="nf">kmeans_pp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">nclusters</span><span class="p">,</span> <span class="n">nrepeat</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sd">"""Repeat the k-means algoritm nrepeat times, picking the optimum.</span>

<span class="sd">    X is an array of of shape (n,m) containing n data points (observations)</span>
<span class="sd">    each of dimension m.</span>
<span class="sd">    nclusters (also commonly referred to as K) is the number of clusters to</span>
<span class="sd">    partition the data into.</span>
<span class="sd">    nrepeat is the number of runs of the k-means algorithm to repeat with</span>
<span class="sd">    different random initial cluster centroids in order to pick the best one.</span>
<span class="sd">    verbose = True/False turns on/off some progress reporting.</span>

<span class="sd">    """</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'iteration | this goodness | best goodness'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'-----------------------------------------'</span><span class="p">)</span>
    <span class="n">best_goodness</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nrepeat</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">nclusters</span><span class="p">)</span>
        <span class="n">this_goodness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">cluster</span><span class="o">-</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
                                    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">clusters</span><span class="p">)])</span>
        <span class="k">if</span> <span class="n">best_goodness</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">best_goodness</span> <span class="o">&gt;</span> <span class="n">this_goodness</span><span class="p">:</span>
            <span class="n">best_goodness</span> <span class="o">=</span> <span class="n">this_goodness</span>
            <span class="n">best_clustering</span> <span class="o">=</span> <span class="n">mu</span><span class="p">,</span> <span class="n">clusters</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">'{:^9d} | {:13g} | {:13g}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">this_goodness</span><span class="p">,</span>
                                                  <span class="n">best_goodness</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">best_clustering</span>
</pre></div>


</div>

</div>
</div>

<footer>
<div class="container">
</div>
</footer>








<script>

var _gaq = _gaq || [['_trackPageview']];
_gaq.unshift(['_setAccount', 'UA-91424547-3']);
(function(d, t) {
    var g = d.createElement(t),
        s = d.getElementsByTagName(t)[0];
    g.async = true;
    g.src = '//www.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g, s);
})(document, 'script');

</script>


</body>
</html>
